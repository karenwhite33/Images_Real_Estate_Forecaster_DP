# -*- coding: utf-8 -*-
"""1_images_download_csv_formatting.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10YjBGadilIVDGUEmcyadnXySoiFFtgct

## Descarga y formatting del CSV de los datos
"""

import cv2
import numpy as np
import pandas as pd
import imageio.v3 as io

from tqdm import tqdm
from typing import Optional, Union

import os
print(os.getcwd())

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv("/content/drive/My Drive/Deep_Learning_2024/Notebooks/data/dataset.csv", sep = ';')
data = data.dropna(subset=['price'])
data

# ¿De qué variables disponemos?
data.columns

"""## Descarga de imágenes"""

def download_images(paths: list,
                    canvas: tuple = (224, 224),
                    nb_channels: int = 3,
                    max_imgs: Optional[int] = None
                    ) -> tuple:
  """ Download a list of images from url adresses, converting them to a specific
  canvas size.

  Args:
    paths: Paths or url adresses from which to load images.
    canvas: Desired image width and height.
    nb_channels: Channels in images (1 for B/W, 3 for RGB).
    max_imgs: Upper threshold in the number of images to download.

  Return:
    a tuple of:
      - image values
      - indices within the paths that were successfull.

  """
  n_images = len(paths) if not max_imgs else max_imgs
  images = np.zeros((n_images, canvas[0], canvas[1], nb_channels),
                       dtype=np.uint8)
  downloaded_idxs = []

  for i_img, url in enumerate(tqdm(paths, total=n_images)):
    if i_img >= n_images:
      break
    try:
        img = io.imread(url)
        img = cv2.resize(img, (canvas[0], canvas[1]))
        downloaded_idxs.append(i_img)
        images[i_img] = img
    except (IOError, ValueError) as e:  # Unavailable url / conversion error
        pass
  return images[downloaded_idxs], downloaded_idxs

"""Normalización y conversión en el rango [0, 1]

**RECORDAD SEÑALAR LA COLUMNA ADECUADA**
"""

images, idxs = download_images(data['picture_url'])
images = images.astype("float32") / 255.
images.shape

# Imprimir dimensiones de las primeras y últimas 5 imágenes
for i, img in enumerate(images):
    if i < 5 or i >= len(images) - 5:
        print(f"Imagen {i+1} dimensiones: {img.shape}")

"""Filtrado acorde en los demás datos"""

# Filtramos el df para quedarnos con las columnas que descargaron efectivamente las imagenes

filtered_data = data.iloc[idxs]
filtered_data

# Contar el número total de filas antes de eliminar duplicados
filas_totales = len(filtered_data)

# Contar el número de filas únicas
filas_unicas = len(filtered_data.drop_duplicates())

# Calcular el número de filas duplicadas
filas_duplicadas = filas_totales - filas_unicas

# Imprimir el resultado
print(f"Se eliminarán {filas_duplicadas} filas duplicadas.")

#El resultado es
#Se eliminarán 3130 filas duplicadas.

# Eliminar filas duplicadas y reiniciar el índice
filtered_data = filtered_data.drop_duplicates().reset_index(drop=True)

#Alinear images a la longitud de filtered_data
images = images[:len(filtered_data)]

# Eliminar el símbolo de dólar y convertir a tipo numérico label 'price'
filtered_data['price'] = filtered_data['price'].replace('[\$,]', '', regex=True).astype(float)
# Ver solo la columna 'price'
print(filtered_data['price'])

"""## Guardado de los datos para posterior uso/selección"""

np.save('images.npy', images)
filtered_data.to_csv('filtered_data.csv', sep=';', index=False)

from google.colab import drive
drive.mount('/content/drive')

!cp images.npy /content/drive/MyDrive/Deep_Learning_2024/Notebooks/data/images_final.npy
!cp filtered_data.csv /content/drive/MyDrive/Deep_Learning_2024/Notebooks/data/filtered_data.csv
!ls -lah images* filtered*  # Comprobación

# Comprobamos a abrirlos de nuevo
saved_data = pd.read_csv("/content/drive/MyDrive/Deep_Learning_2024/Notebooks/data/filtered_data.csv", sep=';')
saved_imgs = np.load("/content/drive/MyDrive/Deep_Learning_2024/Notebooks/data/images_final.npy")
saved_data.shape, saved_imgs.shape

images.min()

images.max()

type(images[0])

images.shape

# Ver las primeras 5 filas de todas las columnas en filtered_data
saved_data.head()

"""##SPLIT DE DATOS EN TRAIN, VALIDATION Y TEST EN DATOS TABULARES E IMÁGENES
---


"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

# Dividir el dataset tabular en train, val y test y obtener los índices
fd_train, fd_test, idx_train, idx_test = train_test_split(
    filtered_data, range(len(filtered_data)), test_size=0.2, random_state=42
)
fd_train, fd_val, idx_train, idx_val = train_test_split(
    fd_train, idx_train, test_size=0.15, random_state=42
)

# Guardar cada conjunto tabular en un único archivo CSV en Google Drive
fd_train.to_csv("/content/drive/MyDrive/Deep_Learning_2024/Notebooks/data/fd_train.csv", sep=';', index=False)
fd_val.to_csv("/content/drive/MyDrive/Deep_Learning_2024/Notebooks/data/fd_val.csv", sep=';', index=False)
fd_test.to_csv("/content/drive/MyDrive/Deep_Learning_2024/Notebooks/data/fd_test.csv", sep=';', index=False)

# Dividir las imágenes usando los índices obtenidos de los datos tabulares
imgs_train = images[idx_train]
imgs_val = images[idx_val]
imgs_test = images[idx_test]

# Guardar los conjuntos de imágenes en archivos .npy
np.save("/content/drive/MyDrive/Deep_Learning_2024/Notebooks/data/imgs_train.npy", imgs_train)
np.save("/content/drive/MyDrive/Deep_Learning_2024/Notebooks/data/imgs_val.npy", imgs_val)
np.save("/content/drive/MyDrive/Deep_Learning_2024/Notebooks/data/imgs_test.npy", imgs_test)

import pandas as pd

# Cargar el último estado de fd_train desde Google Drive
fd_train = pd.read_csv('/content/drive/MyDrive/Deep_Learning_2024/Notebooks/data/fd_train.csv', sep=';')
fd_val = pd.read_csv("/content/drive/MyDrive/Deep_Learning_2024/Notebooks/data/fd_val.csv", sep=';',)
fd_test = pd.read_csv("/content/drive/MyDrive/Deep_Learning_2024/Notebooks/data/fd_test.csv", sep=';')

"""## C.  Normalización y redimensionado de datos numéricos y categóricos DATOS TABULARES FD_TRAIN"""

pd.set_option('display.max_rows', None) # para mostrar todas las filas
pd.set_option('display.max_columns', None) # para mostrar todas las columnas

#revisar valores únicos
fd_train.apply(lambda x: len(x.unique()))

#Revisar que tipo de datos tenemos y nulls
data_overview = pd.DataFrame({
    'Data Type': fd_train.dtypes,
    'Total Nulls': fd_train.isnull().sum()
})

# Mostrar el DataFrame
data_overview

# Identificar columnas con más del 50% de valores nulos
high_null_columns = data_overview[data_overview['Total Nulls'] > (0.5 * len(fd_train))]
high_null_columns

"""DROPS DE COLUMNAS QUE NO FUNCIONAN PARA LO QUE REQUERIMOS

100% NULLS:
1. Eliminare *'calendar_update'* por que tiene 100 nulls% y ya tenemos otras 2 features de 'neighbourhood' por lo que *'neighbourhood_group_cleansed'* no hace falta.

2. Bathrooms' y 'bedrooms' se trabajaran mas adelante pues se contrastaran con 'bathrooms_text' y 'beds'.

OTRAS VARIABLES QUE NO APORTAN A PRICE:
3.
  *scrape_id:* no aporta información sobre la propiedad.

  *last_scraped:* no aporta información sobre la propiedad.

  *source:* no aporta información sobre la propiedad.

  *host_id*: aporta información sobre el dueño, pero no sobre la propiedad *

  *host_url:* no aporta información sobre la propiedad.

  *host_name:* no aporta información sobre la propiedad.

  *host_thumbnail_url:* no aporta información sobre la propiedad.

  *host_location:* no aporta información sobre la propiedad.

  *host_about:* no aporta información sobre la propiedad.

  *host_picture_url:* no aporta información sobre la propiedad.

  *Host_neighbourhood:* no aporta información sobre la propiedad.

  *host_listings_count:* no aporta información sobre la propiedad.

  *maximum_minimum_nights,minimum_maximum_nights,maximum_maximum_nights,minimum_nights_avg_ntm,maximum_nights_avg_ntm:* redundantes con maximun y minimu_nights

  *has_availability,availability_30,availability_60,availability_90:* es redundante se deja solo availability_365

  *calendar_last_scraped:* no aporta información sobre la propiedad.

  *license:* no aporta información valiosa a precio.

  *calculated_host_listings_count_entire_homes,calculated_host_listings_count_private_rooms,calculated_host_listings_count_shared_rooms, calculated_host_listings_count:* son redudantes contador relacionado con el dueño; no aporta información sobre la propiedad.

  *first_review:* no aporta información nueva; ya hay un dato de antiguedad del dueño con host_since

  *last_review:* no aporta información nueva.


"""

# Hacemos un drop de todas las columnas que no aportan información relevante a precio o inmueble

not_useful_columns = [
    'first_review', 'last_review', 'calculated_host_listings_count_entire_homes',
    'calculated_host_listings_count_private_rooms', 'calculated_host_listings_count_shared_rooms',
    'calculated_host_listings_count', 'license', 'calendar_last_scraped',
    'has_availability', 'availability_30', 'availability_60', 'availability_90',
    'maximum_minimum_nights', 'minimum_maximum_nights', 'maximum_maximum_nights',
    'minimum_nights_avg_ntm', 'maximum_nights_avg_ntm', 'host_listings_count',
    'host_neighbourhood', 'host_picture_url', 'host_about', 'host_location',
    'host_thumbnail_url', 'host_name', 'host_url', 'host_id', 'source',
    'last_scraped', 'scrape_id', 'neighbourhood_group_cleansed', 'calendar_updated'
]

fd_train.drop(not_useful_columns, axis=1, inplace=True)

fd_train.shape

"""##Imputacion de NULLS columnas numéricas:"""

# Identificar columnas numéricas con valores nulos
numeric_columns = fd_train.select_dtypes(include=['float64', 'int64']).columns
numeric_null_columns = numeric_columns[fd_train[numeric_columns].isnull().any()]

# Contar los valores nulos en cada una de las columnas numéricas con nulos
numeric_null_counts = fd_train[numeric_null_columns].isnull().sum()
numeric_null_counts

"""#nulls en review_scores_* y reviews_per_month

Usaré la mediana para imputar valores nulos en las columnas de calificaciones (review_scores_*) y frecuencia de revisiones (reviews_per_month) por las siguientes razones:

1. Las calificaciones y frecuencias de revisiones suelen tener una distribución asimétrica. La mediana es más robusta porque representa el valor central de la distribución sin ser influenciada por outliers.

2. Las calificaciones y frecuencias son valores continuos y escalares.
"""

# Imputare review_scores* y reviews_per_month con la mediana

review_columns = ['review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness',
                  'review_scores_checkin', 'review_scores_communication', 'review_scores_location',
                  'review_scores_value', 'reviews_per_month']

# Imputación de cada columna con la mediana
for column in review_columns:
    fd_train[column].fillna(fd_train[column].median(), inplace=True)

# Imprimir el número de nulos restantes después de la imputación
print("Número de nulos restantes después de la imputación en reviews*:")
print(fd_train[review_columns].isnull().sum())

"""#Imputacion 'bathrooms' y 'bathrooms_text'
ontinuamos con 'bathrooms' que est 100% con nulos pero 'bathrooms_text' tiene valores, se puede extraer el número de baños de 'bathrooms_text' y utilizarlo para llenar el valor en 'bathrooms'.
'bathrooms_text' contiene "shared", se agregaria una columna binaria ('bathroom_shared') para indicar si el baño es compartido [1]o privado [0].
"""

import re

# extraemos el número de baños de bathrooms_text
def extract_bathrooms(text):
    if pd.isnull(text):
        return None
    match = re.search(r'(\d+)', text)
    return float(match.group(1)) if match else None

# Crear una nueva columna 'bathrooms_shared' para indicar si el baño es compartido o no
fd_train['bathrooms_shared'] = fd_train['bathrooms_text'].apply(lambda x: 1 if 'shared' in str(x).lower() else 0)

# Imputar bathrooms usando bathrooms_text cuando bathrooms está en nulo
fd_train['bathrooms'] = fd_train['bathrooms'].combine_first(fd_train['bathrooms_text'].apply(extract_bathrooms))

# Imprimir el número de nulos restantes
print("Nulos en 'bathrooms' después de la imputación:", fd_train['bathrooms'].isnull().sum())
print("Nulos en 'bathrooms_shared':", fd_train['bathrooms_shared'].isnull().sum())

# Eliminar bathrooms_text si ya no tiene nulos en bathrooms y bathrooms_shared
if fd_train['bathrooms'].isnull().sum() == 0 and fd_train['bathrooms_shared'].isnull().sum() == 0:
    fd_train.drop(columns=['bathrooms_text'], inplace=True)
    print("'bathrooms_text' ha sido eliminada porque ya no hay nulos en 'bathrooms' y 'bathrooms_shared'.")
else:
    print("'bathrooms_text' no ha sido eliminada porque aún hay nulos en 'bathrooms' o 'bathrooms_shared'.")

"""# Imputar 'beds' que tiene 32 valores nulos

Puedo aplicar la imputación condicional basada en agrupaciones por características:

Usare *'property_type', 'room_type', 'accommodates', 'bathrooms' y la nueva variable creada 'bathrooms_shared'* para formar grupos que puedan representar propiedades similares en términos de cantidad de camas.

Calculare la mediana de beds en cada grupo para obtener el número típico de camas en propiedades similares.

Para cada fila con beds nulo, rellenamos con la mediana correspondiente al grupo al que pertenece.
"""

# Calcular la mediana de 'beds' en cada grupo relevante
fd_train['beds'] = fd_train['beds'].fillna(
    fd_train.groupby(['property_type', 'room_type', 'accommodates', 'bathrooms', 'bathrooms_shared'])['beds'].transform('median')
)

# Imprimir el número de nulos restantes en 'beds' después de la imputación
print("Nulos restantes en 'beds' después de la imputación:", fd_train['beds'].isnull().sum())

# Mostrar la fila completa donde 'beds' tiene un valor nulo
fd_train[fd_train['beds'].isnull()]

"""Como decision informada y revisando el nulo que queda, por las caracterisiticas de ser una habitacion privada que permite 2 invitados y tiene un baño compartido, por lo general tendria 1 cama lo que sugiere una imputacion mas especifica."""

# Imputar el nulo en 'beds' de esta fila específica con 1 cama
fd_train.loc[fd_train['beds'].isnull(), 'beds'] = 1

# Verificar que no quedan nulos
print("Nulos restantes en 'beds' después de la imputación final:", fd_train['beds'].isnull().sum())

"""# Imputar 'bedrooms' que tiene 100% valores nulos

Dado que 'bedrooms' tiene 100% de valores nulos en train, si lo hago por agrupación de variables similares, no se podrá calcular medianas útiles sin algún valor inicial.

Revisaré los valores unicos de las variables clave para imputar 'bedrooms'
"""

# Código para obtener los valores únicos de las variables clave para imputación
print("Valores únicos de 'property_type':", fd_train["property_type"].unique())
print("Valores únicos de 'room_type':", fd_train["room_type"].unique())
print("Valores únicos de 'accommodates':", fd_train["accommodates"].unique())
print("Valores únicos de 'beds':", fd_train["beds"].unique())
print("Valores únicos de 'bathrooms_shared':", fd_train["bathrooms_shared"].unique())

"""##Estrategia para imputar 'bedrooms' con reglas

Segun los valores únicos, definiré reglas basadas en la estructura y lógica de * property_type, room_type, accommodates, beds, y bathrooms_shared*. Estas reglas garantizarán una imputación precisa de 'bedrooms' y reproducible en train, validation, y test.

***A. Regla Basada en *room_type:* ***

Entire home/apt: Como es una unidad completa, es lógico asumir que el número de bedrooms está relacionado con la capacidad (accommodates).
La regla será:

Si accommodates ≤ 2 → bedrooms = 1.
Si 3 ≤ accommodates ≤ 4 → bedrooms = 2.
Si 5 ≤ accommodates ≤ 6 → bedrooms = 3.
Para accommodates > 6, aplicamos bedrooms = np.ceil(accommodates / 2).

Private room: La mayoría de las propiedades en esta categoría tienen una sola habitación. Imputaremos:

bedrooms = 1 (regla directa).
Hotel room y Shared room: Dado que son tipos de habitación sin división de áreas de descanso separadas, asignamos:

bedrooms = 1.

***B. Regla Complementaria con *property_type:* ***

Algunas propiedades específicas (como Entire condo, Entire rental unit, Entire guest suite) generalmente siguen las mismas reglas que Entire home/apt. Sin embargo, el uso de property_type sirve como validación de los valores que asigno asegurando que estoy cubriendo correctamente toda la variedad de propiedades.

***C. Regla Basada en *bathrooms_shared:* ***

Si el baño es compartido (bathrooms_shared = 1) y room_type es Private room, esto confirma que es una sola habitación.
Entonces:
bedrooms = 1.
Para los demás tipos, bathrooms_shared no afecta bedrooms directamente, pero se considera para validar los valores que se asignan en otros tipos.

Asi usó reglas claras que se aplican de forma determinística en train, luego en validation, y test, garantizando la consistencia sin riesgo de introducir sesgo.
"""

import numpy as np

# Función de imputación de 'bedrooms' basada en reglas
def imputar_bedrooms(df):
    for index, row in df.iterrows():
        if row['room_type'] == "Entire home/apt":
            # Reglas para 'Entire home/apt'
            if row['accommodates'] <= 2:
                df.at[index, 'bedrooms'] = 1
            elif 3 <= row['accommodates'] <= 4:
                df.at[index, 'bedrooms'] = 2
            elif 5 <= row['accommodates'] <= 6:
                df.at[index, 'bedrooms'] = 3
            else:
                df.at[index, 'bedrooms'] = np.ceil(row['accommodates'] / 2)

        elif row['room_type'] == "Private room":
            # Regla para 'Private room'
            df.at[index, 'bedrooms'] = 1

        elif row['room_type'] in ["Hotel room", "Shared room"]:
            # Regla para 'Hotel room' y 'Shared room'
            df.at[index, 'bedrooms'] = 1

    return df

# Aplicar la función a fd_train
fd_train = imputar_bedrooms(fd_train)

# Verificar nulos restantes en 'bedrooms' después de la imputación en train
print("Nulos restantes en 'bedrooms' después de la imputación en fd_train:", fd_train['bedrooms'].isnull().sum())

'''
# Aplicar la función en fd_validation y fd_test para garantizar consistencia
fd_validation = imputar_bedrooms(fd_validation)
fd_test = imputar_bedrooms(fd_test)

# Verificar nulos restantes en fd_validation y fd_test
print("Nulos restantes en 'bedrooms' en fd_validation después de la imputación:", fd_validation['bedrooms'].isnull().sum())
print("Nulos restantes en 'bedrooms' en fd_test después de la imputación:", fd_test['bedrooms'].isnull().sum())'''

# Mostrar 5 ejemplos de los valores imputados en 'bedrooms'
print(fd_train[fd_train['bedrooms'].notnull()].sample(5))

# Identificar columnas numéricas con valores nulos
numeric_columns = fd_train.select_dtypes(include=['float64', 'int64']).columns
numeric_null_columns = numeric_columns[fd_train[numeric_columns].isnull().any()]

# Contar los valores nulos en cada una de las columnas numéricas con nulos
numeric_null_counts = fd_train[numeric_null_columns].isnull().sum()
numeric_null_counts

"""###Imputacion de NULLS columnas catégoricas:"""

# Identificar columnas categóricas con valores nulos
categorical_columns = fd_train.select_dtypes(include=['object']).columns
categorical_null_columns = categorical_columns[fd_train[categorical_columns].isnull().any()]

# Contar los valores nulos en cada una de las columnas categóricas con nulos
categorical_null_counts = fd_train[categorical_null_columns].isnull().sum()
print(categorical_null_counts)

# Ver todos los tipos de datos y el número de columnas de cada tipo en fd_train
data_types = fd_train.dtypes.value_counts()
print(data_types)

fd_train['host_response_time'].unique()

# Convertir 'host_response_rate' y 'host_acceptance_rate' de texto a valores numéricos quitando el % y dividiendo el numero por 100
fd_train['host_response_rate'] = fd_train['host_response_rate'].str.rstrip('%').astype(float) / 100
fd_train['host_acceptance_rate'] = fd_train['host_acceptance_rate'].str.rstrip('%').astype(float) / 100

# Imputar 'host_response_time' basado en reglas condicionales en función de 'host_is_superhost' y 'host_response_rate'
def imputar_host_response_time(row):
    if pd.isnull(row['host_response_time']):
        # Si el anfitrión es superhost o tiene una tasa de respuesta alta, asignar respuestas rápidas
        if row['host_is_superhost'] == 't' or (row['host_response_rate'] is not None and row['host_response_rate'] >= 0.9):
            return 'within an hour'
        # Si el anfitrión tiene una tasa de respuesta media
        elif row['host_response_rate'] is not None and 0.5 <= row['host_response_rate'] < 0.9:
            return 'within a day'
        # Si el anfitrión tiene una tasa de respuesta baja o es nulo
        else:
            return 'a few days or more'
    return row['host_response_time']

# Aplicar la imputación condicional a 'host_response_time'
fd_train['host_response_time'] = fd_train.apply(imputar_host_response_time, axis=1)

# Imputar 'host_response_rate' y 'host_acceptance_rate' basado en la media dentro de los grupos de 'host_is_superhost'
fd_train['host_response_rate'] = fd_train.groupby('host_is_superhost')['host_response_rate'].transform(lambda x: x.fillna(x.mean()))
fd_train['host_acceptance_rate'] = fd_train.groupby('host_is_superhost')['host_acceptance_rate'].transform(lambda x: x.fillna(x.mean()))

# Imputar 'host_is_superhost' con el valor más frecuente (moda) en el dataset
fd_train['host_is_superhost'].fillna(fd_train['host_is_superhost'].mode()[0], inplace=True)

# Verificar nulos restantes en las columnas host_*
print("Nulos restantes en host_response_time:", fd_train['host_response_time'].isnull().sum())
print("Nulos restantes en host_response_rate:", fd_train['host_response_rate'].isnull().sum())
print("Nulos restantes en host_acceptance_rate:", fd_train['host_acceptance_rate'].isnull().sum())
print("Nulos restantes en host_is_superhost:", fd_train['host_is_superhost'].isnull().sum())

#Para los nulos restantes
# Calcular la media de 'host_response_rate' y 'host_acceptance_rate' por cada grupo de 'host_response_time' para anfitriones no superhosts
response_rate_mean_by_time = fd_train[fd_train['host_is_superhost'] == 'f'].groupby('host_response_time')['host_response_rate'].mean().to_dict()
acceptance_rate_mean_by_time = fd_train[fd_train['host_is_superhost'] == 'f'].groupby('host_response_time')['host_acceptance_rate'].mean().to_dict()

# Imputar valores nulos en 'host_response_rate' y 'host_acceptance_rate' en función de 'host_response_time' y 'host_is_superhost'
fd_train['host_response_rate'] = fd_train.apply(
    lambda row: response_rate_mean_by_time[row['host_response_time']] if pd.isnull(row['host_response_rate']) and row['host_is_superhost'] == 'f' else row['host_response_rate'],
    axis=1
)

fd_train['host_acceptance_rate'] = fd_train.apply(
    lambda row: acceptance_rate_mean_by_time[row['host_response_time']] if pd.isnull(row['host_acceptance_rate']) and row['host_is_superhost'] == 'f' else row['host_acceptance_rate'],
    axis=1
)

# Verificar nulos restantes después de la imputación
print("Nulos restantes en host_response_rate:", fd_train['host_response_rate'].isnull().sum())
print("Nulos restantes en host_acceptance_rate:", fd_train['host_acceptance_rate'].isnull().sum())

"""# Función para aplicar las reglas de imputación guardadas en validation y test
def imputar_host_val_test(df):
    # Imputar 'host_response_time' basado en la moda guardada por 'host_is_superhost'
    df['host_response_time'] = df.apply(
        lambda row: response_time_mode.get(row['host_is_superhost'], "within a day") if pd.isnull(row['host_response_time']) else row['host_response_time'],
        axis=1
    )

    # Imputar 'host_response_rate' y 'host_acceptance_rate' basado en la media guardada por 'host_response_time'
    df['host_response_rate'] = df.apply(
        lambda row: response_rate_mean_by_time.get(row['host_response_time'], 0.5) if pd.isnull(row['host_response_rate']) else row['host_response_rate'],
        axis=1
    )
    df['host_acceptance_rate'] = df.apply(
        lambda row: acceptance_rate_mean_by_time.get(row['host_response_time'], 0.5) if pd.isnull(row['host_acceptance_rate']) else row['host_acceptance_rate'],
        axis=1
    )

    # Imputar 'host_is_superhost' con el valor más común
    df['host_is_superhost'].fillna(fd_train['host_is_superhost'].mode()[0], inplace=True)

    return df

# Aplicar la función en fd_validation y fd_test
fd_validation = imputar_host_val_test(fd_validation)
fd_test = imputar_host_val_test(fd_test)

# Verificar nulos restantes en validation y test
print("Nulos restantes en host_response_time en fd_validation:", fd_validation['host_response_time'].isnull().sum())
print("Nulos restantes en host_response_rate en fd_validation:", fd_validation['host_response_rate'].isnull().sum())
print("Nulos restantes en host_acceptance_rate en fd_validation:", fd_validation['host_acceptance_rate'].isnull().sum())
print("Nulos restantes en host_is_superhost en fd_validation:", fd_validation['host_is_superhost'].isnull().sum())

print("Nulos restantes en host_response_time en fd_test:", fd_test['host_response_time'].isnull().sum())
print("Nulos restantes en host_response_rate en fd_test:", fd_test['host_response_rate'].isnull().sum())
print("Nulos restantes en host_acceptance_rate en fd_test:", fd_test['host_acceptance_rate'].isnull().sum())
print("Nulos restantes en host_is_superhost en fd_test:", fd_test['host_is_superhost'].isnull().sum())

""""

# Identificar columnas categóricas con valores nulos
categorical_columns = fd_train.select_dtypes(include=['object']).columns
categorical_null_columns = categorical_columns[fd_train[categorical_columns].isnull().any()]

# Contar los valores nulos en cada una de las columnas categóricas con nulos
categorical_null_counts = fd_train[categorical_null_columns].isnull().sum()
print(categorical_null_counts)













"""Así, por ejemplo, cargamos y visualizamos la distribución de las etiquetas:"""

saved_data['price'].hist(bins=50)

"""import matplotlib.pyplot as plt

# Ver el resumen estadístico para entender el rango de 'price'
print(saved_data['price'].describe())

# Filtrar valores extremos (puedes ajustar el umbral según tus datos)
filtered_prices = saved_data[saved_data['price'] < 3000]['price']  # Ajusta 1000 a un valor razonable según tus datos

# Crear el histograma con un número de bins más bajo para mejor visualización
plt.figure(figsize=(10, 6))
filtered_prices.hist(bins=30)
plt.xlabel('Price')
plt.ylabel('Frequency')
plt.title('Distribución de precios (con valores filtrados)')
plt.show()

Y podemos dividir por porcentajes, para crear clases, como por ejemplo:
"""

saved_data['price'].describe()

"""y_class = []
for y in saved_data['price']:
  if y <= 32:
    y_class.append(0) # Rango más barato
  elif 32 < y <= 55:
    y_class.append(1) # Rango intermedio
  else:
    y_class.append(2)
y_class = pd.Series(y_class)
y_class.hist(bins=3)"""